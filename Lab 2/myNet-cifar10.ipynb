{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File:        myNet-cifar10.ipynb\n",
    "# Author:      Edward Hanson (eth20)\n",
    "# Description: NN model for problem 5 of HW 2; CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms # useful library for preprocessing\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Import matplotlib dependencies\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Additional dependencies for file manipulation\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network module: myNet (by Edward Hanson)\n",
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1), # in_channels, out_channels, kernel_size, stride=1, padding=1\n",
    "            nn.BatchNorm2d(8), # insert batch normalization BEFORE ReLU\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 32, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # bottleneck\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Linear(4 * 4 * 64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer14 = nn.Linear(256, 10)\n",
    "        # CrossEntropyLoss() loss function already implements nn.Softmax(), so no need to include here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.layer13(out)\n",
    "        y_pred = self.layer14(out)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VAL_BATCH_SIZE = 100\n",
    "INITIAL_LR = 0.035\n",
    "MOMENTUM = 0.9\n",
    "REG = 3e-4\n",
    "EPOCHS = 100\n",
    "\n",
    "DECAY_EPOCHS = 2\n",
    "DECAY = 0.98\n",
    "\n",
    "DATAROOT = \"./data\"\n",
    "CHECKPOINT_PATH = \"./saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data preprocessing\"\"\"\n",
    "# Specify preprocessing function.\n",
    "# Reference mean/std value for \n",
    "\n",
    "# Data augmentation for training\n",
    "# ToTensor() HAS to be placed after all PIL transforms\n",
    "transform_train = transforms.Compose([ # Compose multiple transformations together\n",
    "    transforms.RandomHorizontalFlip(), # probability=0.5\n",
    "    transforms.ToTensor(), # convert np array to torch tensor; also normalizes array to [0,1]\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)) # standardize array; mean , std (for each channel)\n",
    "    ])\n",
    "    \n",
    "# Data augmentation for inference mode\n",
    "transform_val = transforms.Compose([ \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Call the dataset Loader\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=1) # applies batch shuffle here\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU...\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = myNet()\n",
    "net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint: ./saved_model/model_curr.h5\n",
      "Starting from epoch 97 \n",
      "Starting from learning rate 0.013271:\n"
     ]
    }
   ],
   "source": [
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model/modelMyNet.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-30 23:41:16.877493\n",
      "Epoch 97:\n",
      "704\n",
      "Initial loss = tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Training loss: 0.4012, Training accuracy: 0.8662\n",
      "2019-09-30 23:41:46.536905\n",
      "Validation...\n",
      "Validation loss: 0.5991, Validation accuracy: 0.8052\n",
      "Saving ...\n",
      "2019-09-30 23:41:47.746844\n",
      "Epoch 98:\n",
      "704\n",
      "Training loss: 0.3108, Training accuracy: 0.8951\n",
      "2019-09-30 23:42:09.917729\n",
      "Validation...\n",
      "Validation loss: 0.5252, Validation accuracy: 0.8290\n",
      "Current learning rate has decayed to 0.013006\n",
      "Saving ...\n",
      "2019-09-30 23:42:11.238406\n",
      "Epoch 99:\n",
      "704\n",
      "Training loss: 0.1379, Training accuracy: 0.9534\n",
      "2019-09-30 23:42:33.730768\n",
      "Validation...\n",
      "Validation loss: 0.3828, Validation accuracy: 0.8860\n",
      "Saving ...\n",
      "Optimization finished.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Start the training/validation process\"\"\"\n",
    "# The process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "initialPass = True\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # Train the training dataset for 1 epoch.\n",
    "    print(len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device [needs to be consistent with the device that model is loaded in]\n",
    "        inputs = inputs.to(device=device, dtype=torch.float)\n",
    "        targets = targets.to(device=device, dtype=torch.long)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        # Calculate predicted labels\n",
    "        # Outputs of neurons for each input listed across dimension 1\n",
    "        # Dimension 0 lists neuron outputs for all inputs\n",
    "        # Neuron with largest output value is understood as y_pred\n",
    "        # Max outputs (value, index) -- in this case, the index (ie neuron ID) is what we're after\n",
    "        _, predicted = outputs.max(1)\n",
    "        # Calculate accuracy\n",
    "        total_examples += TRAIN_BATCH_SIZE\n",
    "        correct_examples += (targets == predicted).sum() # .sum() all values that match between targets and predicted\n",
    "\n",
    "        if initialPass == True:\n",
    "            print(\"Initial loss = \" + str(loss))\n",
    "            initialPass = False\n",
    "        train_loss += loss\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = train_loss.float() / (batch_idx + 1)\n",
    "        pass\n",
    "    avg_acc = correct_examples.float() / total_examples # need .float() since both are ints by default\n",
    "    train_accs.append(avg_acc)\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    ## Validation process ##\n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device=device, dtype=torch.float)\n",
    "            targets = targets.to(device=device, dtype=torch.long)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Calculate accuracy\n",
    "            total_examples += VAL_BATCH_SIZE\n",
    "            correct_examples += (targets == predicted).sum()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss.float() / len(valloader)\n",
    "    avg_acc = correct_examples.float() / total_examples\n",
    "    val_accs.append(avg_acc)\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate *= DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_learning_rate\n",
    "            \n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'modelMyNet.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results\n",
    "with open('report_data/MYNET7_train_acc_curr.txt', 'w') as filehandle:\n",
    "    for val in train_accs:\n",
    "        filehandle.write('%s\\n' % val)\n",
    "with open('report_data/MYNET7_val_acc_curr.txt', 'w') as filehandle:\n",
    "    for val in val_accs:\n",
    "        filehandle.write('%s\\n' % val)\n",
    "\n",
    "# plot results \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(train_accs)), train_accs)\n",
    "ax.plot(range(len(val_accs)), val_accs)\n",
    "ax.set_xlabel('Epoch', fontsize = 'large')\n",
    "ax.set_ylabel('Accuracy', fontsize = 'large')\n",
    "ax.set_title('myNet')\n",
    "ax.legend(['Train Accuracy','Val Accuracy'], loc=4)\n",
    "ax.grid(True)\n",
    "#plt.xlim(80,100)\n",
    "#plt.ylim(0.65,0.82)\n",
    "plt.show()\n",
    "\n",
    "print(\"Max Validation Accuracy: \"+str(max(val_accs)))\n",
    "print(\"Final Training Accuracy: \"+str(train_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Apply model to testset data\"\"\"\n",
    "# Load testset\n",
    "testset = np.float64(np.load('cifar10-batches-images-test.npy'))\n",
    "# normalize to [0,1]\n",
    "testset *= (1.0 / testset.max())\n",
    "# standardize based on mean and std\n",
    "for i in range(len(testset)):\n",
    "    for j in range(len(testset[i])):\n",
    "        for k in range(len(testset[i][j])):\n",
    "            for l in range(len(testset[i][j][k])):\n",
    "                if l == 0:\n",
    "                    testset[i][j][k][l] = (testset[i][j][k][l] - 0.4914) / 0.2023\n",
    "                if l == 1:\n",
    "                    testset[i][j][k][l] = (testset[i][j][k][l] - 0.4822) / 0.1994\n",
    "                if l == 2:\n",
    "                    testset[i][j][k][l] = (testset[i][j][k][l] - 0.4465) / 0.2010\n",
    "# Input format has to be [N,C,W,H]\n",
    "testset = np.swapaxes(testset,2,3)\n",
    "testset = np.swapaxes(testset,1,2)\n",
    "testset = torch.from_numpy(testset)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8d005f61ef1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Copy inputs to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate testset results\n",
    "# Disable gradient during test\n",
    "predictions = []\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs in testloader:\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device=device, dtype=torch.float)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output from the DNN.\n",
    "        outputs = net(inputs)\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        #print(outputs)\n",
    "        predictions.append(predicted.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save testset results\n",
    "with open('testpredictions.csv',mode='w') as outfile:\n",
    "    predictions_writer = csv.writer(outfile, delimiter=',')\n",
    "    predictions_writer.writerow(['Id','Category'])\n",
    "    for idx, output in enumerate(predictions):\n",
    "        predictions_writer.writerow([idx,output])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
